<!DOCTYPE html>
<!--<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>-->
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Multi-Modal Grounded Planning and Efficient Replanning For Learning Embodied Agents with a Few Examples">
  <meta name="keywords" content="Embodied AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multi-Modal Grounded Planning and Efficient Replanning For Learning Embodied Agents with a Few Examples</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9BWYESK2L9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-9BWYESK2L9');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://twoongg.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://twoongg.github.io/projects/realfred/">
            ReALFRED
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/CL-ALFRED">
            CL-ALFRED
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/CAPEAM">
            CAPEAM
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/MCR-Agent">
            MCR-Agent
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/ABP">
            ABP
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/MOCA">
            MOCA
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered is-full-width">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size:45px">
            Multi-Modal Grounded Planning and Efficient Replanning For Learning Embodied Agents with a Few Examples
          </h1>
          <h1 class="title is-4 publication-title">
            AAAI 2025 (Oral)
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://twoongg.github.io">Taewoong Kim</a>,
            </span>
            <span class="author-block">
              <a href="https://bhkim94.github.io">Byeonghwi Kim</a>,
            </span>
            <span class="author-block">
              <a href="https://ppolon.github.io">Jonghyun Choi</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Seoul National University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://to appear"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (to appear)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.17288"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/snumprlab/flare"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src='static/figures/teaser.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:70%;max-width:70%">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Few-shot Language with environmental Adaptive Replanning Embodied agent (FLARE)</span>
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Learning a perception and reasoning module for robotic assistants to plan steps to perform complex tasks based on natural language instructions often requires large free-form language annotations, especially for short high-level instructions. To reduce the cost of annotation, large language models (LLMs) are used as a planner with few data. However, when elaborating the steps, even the state-of-the-art planner that uses LLMs mostly relies on linguistic common sense, often neglecting the status of the environment at command reception, resulting in inappropriate plans. To generate plans grounded in the environment, we propose FLARE (Few-shot Language with environmental Adaptive Replanning Embodied agent), which improves task planning using both language command and environmental perception. As language instructions often contain ambiguities or incorrect expressions, we additionally propose to correct the mistakes using visual cues from the agent. The proposed scheme allows us to use a few language pairs thanks to the visual cues and outperforms state-of-the-art approaches. Our code is available at https://github.com/snumprlab/flare.
            </p>
          </div>
          <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/HPg6dNcnDJU?si=vuzED2rwJbrhwooU"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </div>
        </div>
      </div>
</section>




<section class="section">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">Few-shot Language with Environmental Adaptive Replanning Embodied Agent</h2>
    <div class="content has-text-justified">
      <p>
        The state-of-the-art embodied agent requires extensive data annotation and often generates ungrounded or impractical plans due to language ambiguity.
        To address these issues, we propose FLARE, which combines <b>visual and language</b> inputs to generate executable plans, and <b>adaptively updates</b> plans based on visual observations of the environment.
      <p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <img src="static/figures/architecture.png">
        </div>
      </div>
    </div>


    <h3 class="title is-4">Multi-Modal Planner</h3>
      <div class="columns is-centered">
        <div class="column is-two-quarters">
          <div class="content has-text-justified">
            <p>
              To generate plans using LLMs, previous works only considered linguistic similarity between tasks when selecting relevant examples for few-shot learning. Our Multi-Modal Planner improves upon this by considering both language instruction and visual observations to find more contextually appropriate examples, ensuring the generated plans are grounded in the current environment state.
            </p>
            <p>
              To efficiently represent task sequences, we structure each subgoal as a triplet of [Action, Object, Location]. This compact representation reduces the total instruction length while maintaining all necessary information for task execution.
            </p>
          </div>
        </div>
        <div class="column is-two-quarter">
          <img src='static/figures/MMP.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
        </div>
      </div>

    <h3 class="title is-4">Environment Adaptive Replanning</h3>
      <div class="columns is-centered">
        <div class="column is-two-quarters">
          <div class="content has-text-justified">
            <p>
              Even with LLM-based planning, agents often fail when encountering objects they haven't learned during training, due to language variations (e.g., "couch" vs "sofa"). Our Environment Adaptive Replanning addresses this by monitoring and maintaining a list of all detected objects during task execution.
            </p>
            <p>
              When the agent fails to find a target object, EAR automatically identifies and substitutes it with the most semantically similar object from the observed list, using language-based similarity measurements between object names. This enables the agent to adapt its plans in real-time and continue task execution even with unfamiliar objects.
            </p>
          </div>
        </div>
        <div class="column is-two-quarter">
          <img src='static/figures/EAR.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
        </div>
      </div>
  </div>

</section>



<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <h2 class="title is-3">Results</h2>
      <div class="content has-text-justified">
        <p>
          We evaluate the effectiveness of our FLARE in the <span style="color: #305fac"><a href="https://askforalfred.com/">ALFRED</a></span> benchmark. It requires agents
          to complete household tasks based on language instructions and egocentric observations within interactive 3D environments.
          Both validation and test sets include seen and unseen scenarios, where the seen scenario is part of the training data, while the unseen scenario represents a new and unfamiliar environment for evaluation.
          To evaluate the efficiency of FLARE where human language pairs are scarce, we followed the same few-shot
          setting(0.5%) as in the previous work, <span style="color: #305fac"><a href="https://dki-lab.github.io/LLM-Planner/">LLM-Planner</a></span>.
          For a fair comparison with the previous methods, we use the
          same number of examples work, LLM-Planner (<em>i.e.</em>, 100 examples). The selected 100 examples contain all 7 task types
          for fair representations of 21,023 training examples.
          For evaluation, we follow the same evaluation protocol as
          ALFRED. The primary metric is a success rate
          (SR), measuring the percentage of completed tasks. A goalcondition success rate (GC) measures the percentage of satisfied goal conditions. Furthermore, we assess the efficiency
          of agents penalizing SR and GC (<em>i.e.</em>, PLWSR and PLWGC)
          with the path length of a trajectory taken by the agents.
        <p>
        <p>
          For more details, please check out the paper.
        </p>
        <br>
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <img src="static/tables/results.png">
            <h5>Comparison with state-of-the-art methods</h5>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <video id="teaser" autoplay controls muted loop playsinline height="100%">
        <source src="static/video/flare-alfred-quali-NoVoice-compressed.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  
  <div class="container is-max-widescreen">
  </div>
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <figure>  
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="static/video/1_block_good.mp4"
            type="video/mp4">
          </video>
        </figure>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="static/video/3_block.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="static/video/8.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="static/video/2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>






<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{kim2025multimodal,
      author    = {Kim, Taewoong and Kim, Byeonghwi and Choi, Jonghyun},
      title     = {Multi-Modal Grounded Planning and Efficient Replanning For Learning Embodied Agents with A Few Examples},
      booktitle = {AAAI},
      year      = {2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          The website template is from <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Default Statcounter code for flare
https://twoongg.github.io/projects/flare/ -->
<script type="text/javascript">
  var sc_project=13068319; 
  var sc_invisible=1; 
  var sc_security="3390d8c2"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js"
  async></script>
  <noscript><div class="statcounter"><a title="Web Analytics
  Made Easy - Statcounter" href="https://statcounter.com/"
  target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/13068319/0/3390d8c2/1/"
  alt="Web Analytics Made Easy - Statcounter"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->  
</body>
</html>
